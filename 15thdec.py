# -*- coding: utf-8 -*-
"""15thDec

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dIeQ0VHKnOHK0JzsZPyaNgaJ0cF79U4T
"""

import requests

base_url = "http://books.toscrape.com/index.html"

home_page = requests.get(base_url)

if home_page.status_code == 200:
  print("SUCCESS")
else:
  print(f"FAILED, status code: {home_page.status_code}")

import numpy as np

x = np.ones((5,5))
x
x[1:-1,1:-1] = 0
x

from bs4 import BeautifulSoup

soup = BeautifulSoup(markup=home_page.content, parser="html.parser")

books = soup.find_all(name="li", class_="col-xs-6 col-sm-4 col-md-3 col-lg-3")
len(books)

book = books[1]
book

book_url = book.findChild(name="a").get("href")
book_url

from urllib.parse import urljoin

book_url = urljoin(base_url, book_url)
book_url

book_info = requests.get(book_url).content
book_soup = BeautifulSoup(markup=book_info, parser="html.parser")

name = book_soup.find(name="h1").getText()
name

book_table_data = book_soup.find_all(name="tr")
len(book_table_data)

book_data = {}
for row in book_table_data:
  key = row.find(name="th").getText()
  value = row.find(name="td").getText()
  book_data[key] = value

book_data

def scrape_book(book_url):
  book_info = requests.get(book_url).content
  book_soup = BeautifulSoup(markup=book_info, parser="html.parser")
  book_data = {}
  # getting name
  name = book_soup.find(name="h1").getText()
  book_data['name'] = name
  # getting other data
  book_table_data = book_soup.find_all(name="tr")
  for row in book_table_data:
    key = row.find(name="th").getText()
    value = row.find(name="td").getText()
    book_data[key] = value
  # let's also keep the url of book in final result
  book_data['url'] = book_url
  return book_data
# let's test this
scrape_book(book_url)

page_url = "https://books.toscrape.com/catalogue/page-i.html"
page_content = requests.get(page_url).content
page_soup = BeautifulSoup(markup=page_content, parser="html.parser")
page_books = soup.find_all(name="li", class_="col-xs-6 col-sm-4 col-md-3 col-lg-3")
print(len(page_books))

books_data = []
for book in page_books:
  book_url = book.findChild(name="a").get("href")
  # converting relative URL to absolute
  book_url = urljoin(base_url, book_url)
  book_data = scrape_book(book_url)
  books_data.append(book_data)
books_data[:3]

def scrape_page(page_url):
  books_data = []
  page_content = requests.get(page_url).content
  page_soup = BeautifulSoup(markup=page_content, parser="html.parser")
  page_books = soup.find_all(name="li", class_="col-xs-6 col-sm-4 col-md-3 col-lg-3")
  for book in books:
    book_url = book.findChild(name="a").get("href")
    book_url = urljoin(base_url, book_url)
    book_data = scrape_book(book_url)
    books_data.append(book_data)
  return books_data

page_count = 1
data = []
while True:
  page_url = f"https://books.toscrape.com/catalogue/page-{page_count}.html"
  status = requests.get(page_url).status_code
  # break the loop if we exceed the total page count
  if status == 404:
    break
  page_data = scrape_page(page_url)
  data.extend(page_data) # do not use .append() since the function returns a list
  print(f"Page: {page_count} is SUCCESSFULLY scraped")
  page_count += 1

page_count = 1
data = []
while True:
  page_url = f"https://books.toscrape.com/catalogue/page-{page_count}.html"
  status = requests.get(page_url).status_code
  # break the loop if we exceed the total page count
  if status == 404 or page_count == 6:
    break
  page_data = scrape_page(page_url)
  data.extend(page_data) # do not use .append() since the function returns a list
  print(f"Page: {page_count} is SUCCESSFULLY scraped")
  page_count += 1

book = data[0].copy()
book

float(book['Price (excl. tax)'][1:])

quantity_available = int(book['Availability'].split("(")[-1][:-1].split()[0])
is_available = book['Availability'].split("(")[0].strip()

quantity_available, is_available

def fix(item):
  item['Price (excl. tax)'] = float(item['Price (excl. tax)'][1:])
  item['Price (incl. tax)'] = float(item['Price (incl. tax)'][1:])
  item['Tax'] = float(item['Tax'][1:])
  availability = item.pop('Availability')
  item['is_available'] = True if availability.split("(")[0].strip() == 'In stock' else False
  item['quantity_available'] = int(availability.split("(")[-1][:-1].split()[0])
  return item
formatted_data = [fix(item.copy()) for item in data]
formatted_data[:3]

arr1 = np.array(['Ram','Astha','Brahat'])
arr2 = np.array(['Shyam','Kalyan','Naveen'])
arr1 > arr2

import numpy as np
print(np.sort(np.array(['Ram','Astha','Raghavendra'])))